import joblib
import numpy as np
import pandas as pd

def load_pn_model():
    """åŠ è½½Pné¢„æµ‹æ¨¡å‹"""
    try:
        model = joblib.load('best_model.pkl')
        norm_params = joblib.load('normalization_params.pkl')
        feature_info = joblib.load('feature_info.pkl')
        print("âœ… Pné¢„æµ‹æ¨¡å‹åŠ è½½æˆåŠŸ!")
        return model, norm_params, feature_info
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {{e}}")
        return None, None, None

def predict_pn(ppfd, co2, temperature, rb_ratio, model, norm_params):
    """
    é¢„æµ‹Pnå€¼
    
    å‚æ•°:
    - ppfd: å…‰åˆå…‰å­é€šé‡å¯†åº¦ (Î¼molÂ·mâ»Â²Â·sâ»Â¹)
    - co2: äºŒæ°§åŒ–ç¢³æµ“åº¦ (ppm)
    - temperature: æ¸©åº¦ (Â°C)
    - rb_ratio: çº¢è“å…‰æ¯”ä¾‹
    - model: è®­ç»ƒå¥½çš„æ¨¡å‹
    - norm_params: å½’ä¸€åŒ–å‚æ•°
    
    è¿”å›:
    - é¢„æµ‹çš„Pnå€¼
    """
    # å‡†å¤‡è¾“å…¥æ•°æ®
    input_data = np.array([[ppfd, co2, temperature, rb_ratio]])
    
    print(f"ğŸ” è°ƒè¯•ä¿¡æ¯:")
    print(f"  åŸå§‹è¾“å…¥: {input_data[0]}")
    
    # æ ‡å‡†åŒ–
    if norm_params:
        input_norm = (input_data - norm_params['feat_mean']) / norm_params['feat_std']
        print(f"  æ ‡å‡†åŒ–å: {input_norm[0]}")
        print(f"  ç‰¹å¾å‡å€¼: {norm_params['feat_mean']}")
        print(f"  ç‰¹å¾æ ‡å‡†å·®: {norm_params['feat_std']}")
    else:
        input_norm = input_data
        print(f"  æœªæ ‡å‡†åŒ–")
    
    # é¢„æµ‹
    pred_norm = model.predict(input_norm)
    print(f"  æ ‡å‡†åŒ–é¢„æµ‹å€¼: {pred_norm[0]}")
    
    # åæ ‡å‡†åŒ–
    if norm_params:
        prediction = pred_norm * norm_params['target_std'] + norm_params['target_mean']
        print(f"  ç›®æ ‡å‡å€¼: {norm_params['target_mean']}")
        print(f"  ç›®æ ‡æ ‡å‡†å·®: {norm_params['target_std']}")
        print(f"  åæ ‡å‡†åŒ–å: {prediction[0]}")
    else:
        prediction = pred_norm
        print(f"  æœªåæ ‡å‡†åŒ–")
    
    return prediction[0]

def batch_predict(input_data, model, norm_params):
    """
    æ‰¹é‡é¢„æµ‹
    
    å‚æ•°:
    - input_data: å½¢çŠ¶ä¸º(n_samples, 4)çš„æ•°ç»„
    - model: è®­ç»ƒå¥½çš„æ¨¡å‹
    - norm_params: å½’ä¸€åŒ–å‚æ•°
    
    è¿”å›:
    - é¢„æµ‹ç»“æœæ•°ç»„
    """
    # æ ‡å‡†åŒ–
    if norm_params:
        input_norm = (input_data - norm_params['feat_mean']) / norm_params['feat_std']
    else:
        input_norm = input_data
    
    # é¢„æµ‹
    pred_norm = model.predict(input_norm)
    
    # åæ ‡å‡†åŒ–
    if norm_params:
        predictions = pred_norm * norm_params['target_std'] + norm_params['target_mean']
    else:
        predictions = pred_norm
    
    return predictions

def create_sample_data():
    """åˆ›å»ºç¤ºä¾‹æ•°æ®"""
    # æ¨¡æ‹Ÿä¸åŒå…‰ç…§æ¡ä»¶ä¸‹çš„æ•°æ®
    sample_data = [
        [50, 400, 20, 0.5],   # ä½å…‰ç…§
        [200, 400, 22, 0.75], # ä¸­ç­‰å…‰ç…§
        [500, 800, 24, 1.0]   # é«˜å…‰ç…§
    ]
    return sample_data

def validate_training_data(model, norm_params, feature_info):
    """
    éªŒè¯è®­ç»ƒæ•°æ®ï¼Œæ£€æŸ¥æ¨¡å‹æ˜¯å¦èƒ½æ­£ç¡®é¢„æµ‹å·²çŸ¥æ ·æœ¬
    """
    print("\nğŸ” éªŒè¯è®­ç»ƒæ•°æ®...")
    
    # ä»averaged_data.csvä¸­æå–çš„å·²çŸ¥æ ·æœ¬
    known_samples = [
        [50.0, 400.0, 20.0, 0.5, 2.41071015809815],  # å®é™…Pn=2.4107
        [100.0, 400.0, 20.0, 0.5, 4.482195309391792], # å®é™…Pn=4.4822
        [200.0, 400.0, 20.0, 0.5, 7.813590918067845], # å®é™…Pn=7.8136
    ]
    
    print(f"éªŒè¯æ ·æœ¬:")
    for i, sample in enumerate(known_samples):
        input_data = np.array([sample[:4]])  # å‰4ä¸ªæ˜¯ç‰¹å¾
        actual_pn = sample[4]                # ç¬¬5ä¸ªæ˜¯å®é™…Pnå€¼
        
        # é¢„æµ‹
        pred_pn = predict_pn(input_data[0, 0], input_data[0, 1], 
                            input_data[0, 2], input_data[0, 3], 
                            model, norm_params)
        
        # è®¡ç®—è¯¯å·®
        error = abs(pred_pn - actual_pn)
        error_percent = (error / actual_pn) * 100
        
        print(f"  æ ·æœ¬{i+1}: è¾“å…¥{sample[:4]} -> å®é™…Pn={actual_pn:.4f}, é¢„æµ‹Pn={pred_pn:.4f}")
        print(f"    ç»å¯¹è¯¯å·®: {error:.4f}, ç›¸å¯¹è¯¯å·®: {error_percent:.2f}%")
        print()

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # 1. åŠ è½½æ¨¡å‹
    model, norm_params, feature_info = load_pn_model()
    if model is None:
        exit(1)
    
    print(f"æ¨¡å‹ç±»å‹: {feature_info['model_name']}")
    print(f"ç‰¹å¾åˆ—: {feature_info['feature_columns']}")
    print(f"ç›®æ ‡åˆ—: {feature_info['pn_column']}")
    
    # 2. éªŒè¯è®­ç»ƒæ•°æ®
    validate_training_data(model, norm_params, feature_info)
    
    # 3. å•ä¸ªé¢„æµ‹ç¤ºä¾‹
    print("å•ä¸ªé¢„æµ‹ç¤ºä¾‹:")
    pn_pred = predict_pn(200, 400, 22, 0.75, model, norm_params)
    print(f"è¾“å…¥: PPFD=200, CO2=400, T=22, R:B=0.75")
    print(f"é¢„æµ‹Pn: {pn_pred:.4f}")
    
    # 4. æ‰¹é‡é¢„æµ‹ç¤ºä¾‹
    print("\næ‰¹é‡é¢„æµ‹ç¤ºä¾‹:")
    test_data = create_sample_data()
    batch_predictions = batch_predict(np.array(test_data), model, norm_params)
    for i, (data, pred) in enumerate(zip(test_data, batch_predictions)):
        print(f"æ ·æœ¬{i+1}: {data} -> Pn={pred:.4f}")
    
    # 5. ç‰¹å¾é‡è¦æ€§åˆ†æï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
    if hasattr(model, 'feature_importances_'):
        print("\nç‰¹å¾é‡è¦æ€§:")
        for i, importance in enumerate(model.feature_importances_):
            print(f"  {feature_info['feature_columns'][i]}: {importance:.4f}")
    elif hasattr(model, 'coef_'):
        print("\nç‰¹å¾ç³»æ•°:")
        for i, coef in enumerate(model.coef_):
            print(f"  {feature_info['feature_columns'][i]}: {coef:.4f}")
